from causallearn.search.ConstraintBased.PC import pc
from causallearn.utils.GraphUtils import GraphUtils
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from causallearn.utils.cit import chisq
from causallearn.search.ConstraintBased.PC import pc_alg
import networkx as nx
import matplotlib.pyplot as plt


file_path = r"C:\Users\user\Desktop\ì‚°ì¬ë³´í—˜\2ì°¨\df_cleaned.csv"

# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv(file_path, encoding='utf-8-sig')  # í•œê¸€ í¬í•¨ ì‹œ utf-8-sig ê¶Œì¥

drop_cols=[
    "A005A", "A005A03", "A005A04", "A005A05", "A005A06",
    "A005B01", "A005B02", "A005B03", "A005B04", "A005B05", "A005B06",
    "A005C01", "A005C02", "A005C03", "A005C04", "A005C05", "A005C06",
    "A005D01", "A005D02", "A005D03", "A005D04", "A005D05", "A005D06",
    "A005E01", "A005E02", "A005E03", "A005E04", "A005E05", "A005E06",
    "A005F01", "A005F02", "A005F03", "A005F04", "A005F05", "A005F06",
    "A005G01", "A005G02", "A005G03", "A005G04", "A005G05", "A005G06",
    "A005H01", "A005H02", "A005H03", "A005H04", "A005H05", "A005H06",
    "A005I01", "A005I02", "A005I03", "A005I04", "A005I05", "A005I06",
    "A005J01", "A005J02", "A005J03", "A005J04", "A005J05", "A005J06",
    "A005K01", "A005K02", "A005K03", "A005K04", "A005K05", "A005K06",
    "A005L01", "A005L02", "A005L03", "A005L04", "A005L05", "A005L06",
    "A005M01", "A005M02", "A005M03", "A005M04", "A005M05", "A005M06",
    "A005N01", "A005N02", "A005N03", "A005N04", "A005N05", "A005N06",
    "A005O01", "A005O02", "A005O03", "A005O04", "A005O05", "A005O06"
]


df = df.drop(columns=drop_cols, errors='ignore')
# object íƒ€ì… ì—´ë“¤ ì‚­ì œ
object_cols_to_drop = df.select_dtypes(include='object').columns
df = df.drop(columns=object_cols_to_drop)

#í˜„ì¬ ì¼ìë¦¬ ì—¬ë¶€ íŒë‹¨ ì—´ ì œê±°(contamination)
d_cols = [col for col in df.columns if col.startswith('D')]
df = df.drop(columns=d_cols)
#ë§ˆì°¬ê°€ì§€
e_cols = [col for col in df.columns if col.startswith('E')]
df = df.drop(columns=e_cols)

g_cols = [col for col in df.columns if col.startswith('G013')or col.startswith('G012004')]
df = df.drop(columns=g_cols)


h_cols = [col for col in df.columns if col.startswith('H')]
df = df.drop(columns=h_cols)

i_cols = [col for col in df.columns if col.startswith('I')]
df = df.drop(columns=i_cols)




# === 2.5 ê²°ì¸¡ë¥  80% ì´ìƒì¸ ì—´ ì œê±° ===
missing_thresh = 0.8
missing_ratio = df.isnull().mean()
high_missing_cols = missing_ratio[missing_ratio > missing_thresh].index.tolist()
print(f"ğŸ§¹ ê²°ì¸¡ë¥  80% ì´ìƒìœ¼ë¡œ ì œê±°ëœ ì—´ ìˆ˜: {len(high_missing_cols)}")
df = df.drop(columns=high_missing_cols)

# === 2.6 ë¶„ì‚°ì´ ê±°ì˜ ì—†ëŠ” ì—´ ì œê±° ===
low_variance_cols = []
for col in df.select_dtypes(include=[np.number]).columns:
    if df[col].nunique() <= 1:
        low_variance_cols.append(col)
    elif df[col].dtype in ["float32", "float64"] and df[col].var() < 1e-5:
        low_variance_cols.append(col)

print(f"ğŸ§¹ ë¶„ì‚° ê±°ì˜ ì—†ìŒìœ¼ë¡œ ì œê±°ëœ ì—´ ìˆ˜: {len(low_variance_cols)}")
df = df.drop(columns=low_variance_cols)
y_col = 'first_yo'
X = df.drop(columns=[y_col,"second_yo"])
y = df[y_col]
# === 3. ì†Œë¯¼ ìŠ¤íƒ€ì¼ ì „ì²˜ë¦¬ ===
X_encoded = X.copy()
for col in X_encoded.columns:
    if X_encoded[col].dtype in ["float64", "float32"] and X_encoded[col].nunique() > 20:
        X_encoded[col] = X_encoded[col].fillna(X_encoded[col].mean())
    else:
        if X_encoded[col].isnull().any():
            X_encoded[col] = X_encoded[col].fillna(f"question_{col}")
        X_encoded[col] = X_encoded[col].astype(str)
        X_encoded[col] = LabelEncoder().fit_transform(X_encoded[col])

row_thresh = 0.9
X_encoded = X_encoded[X_encoded.isnull().mean(axis=1) < row_thresh]
# === 4. yê°’ ì²˜ë¦¬ ===
if y.dtype == 'bool':
    y = y.astype(int)

from sklearn.feature_selection import SelectKBest, mutual_info_classif


from itertools import combinations
from sklearn.feature_selection import mutual_info_classif
import pandas as pd
import numpy as np
import json

# === 1. ë”•ì…”ë„ˆë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ë¬¸ â†’ í•œê¸€)
with open(r"C:\Users\user\Desktop\ì‚°ì¬ë³´í—˜\variable_dict.json", "r", encoding='utf-8-sig') as f:
    variable_dict = json.load(f)
def to_kor(var): return variable_dict.get(var, var)

# === 2. ê¸°ì¡´ ë³€ìˆ˜ ì¤‘ìš”ë„ ì¸¡ì • (Mutual Info ê¸°ë°˜)
mi_base = mutual_info_classif(X_encoded, y, discrete_features='auto', random_state=42)
mi_base_series = pd.Series(mi_base, index=X_encoded.columns).sort_values(ascending=False)

# === 3. ì¡°í•© ìƒì„±
top_k = 12         # ìƒìœ„ ë³€ìˆ˜ ëª‡ ê°œë¡œ ì¡°í•©í• ì§€
min_comb = 2       # ìµœì†Œ ë³€ìˆ˜ ê°œìˆ˜
max_comb = 4       # ìµœëŒ€ ë³€ìˆ˜ ê°œìˆ˜

top_features = mi_base_series.head(top_k).index.tolist()
combined_features = []

for r in range(min_comb, max_comb + 1):
    combined_features += list(combinations(top_features, r))

print(f"ğŸ” ìƒì„±ëœ ë³µí•© ë³€ìˆ˜ ì¡°í•© ìˆ˜: {len(combined_features)}")

# === 4. ë³µí•© ë³€ìˆ˜ ìƒì„±
X_aug = X_encoded.copy()
new_feature_names = []

for comb in combined_features:
    col_name = "_x_".join(comb)
    X_aug[col_name] = X_encoded[list(comb)].prod(axis=1)
    new_feature_names.append(col_name)

# === 5. Mutual Info ê³„ì‚°
mi_all = mutual_info_classif(X_aug[new_feature_names], y, discrete_features='auto', random_state=42)
mi_result = pd.Series(mi_all, index=new_feature_names).sort_values(ascending=False)

# === 6. í•œê¸€ ì´ë¦„ ë§¤í•‘ í›„ ê²°ê³¼ ì¶œë ¥
print("\nâœ… ìƒìœ„ ë³µí•© ë³€ìˆ˜ ì¡°í•© (í•œê¸€ ë§¤í•‘):")
for i, (feat, score) in enumerate(mi_result.head(15).items(), 1):
    kor_name = " Ã— ".join([to_kor(x) for x in feat.split("_x_")])
    print(f"{i:2d}. {kor_name} â†’ {score:.4f}")
